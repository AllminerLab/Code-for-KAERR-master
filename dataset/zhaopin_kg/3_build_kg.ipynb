{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import numpy as np\n",
    "import random\n",
    "# set random seed\n",
    "random.seed(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user = pd.read_csv('table1_user_processed.txt', sep='\\t')\n",
    "df_jd = pd.read_csv('table2_jd_processed.txt', sep='\\t')\n",
    "df_action = pd.read_csv('table3_action_processed.txt', sep='\\t')\n",
    "\n",
    "df_user.fillna('', inplace=True)\n",
    "df_jd.fillna('', inplace=True)\n",
    "df_action.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_out_dir = 'kg'\n",
    "if not os.path.exists(kg_out_dir):\n",
    "    os.mkdir(kg_out_dir)\n",
    "\n",
    "kg_text_out_dir = 'kg_text'\n",
    "if not os.path.exists(kg_text_out_dir):\n",
    "    os.mkdir(kg_text_out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "city2index = {}\n",
    "\n",
    "df_user['KG_cur_city'] = -1\n",
    "for index, row in df_user.iterrows():\n",
    "    city_id = int(row['live_city_id'])\n",
    "    if city_id not in city2index:\n",
    "        city2index[city_id] = len(city2index)\n",
    "    df_user.loc[index, 'KG_cur_city'] = city2index[city_id]\n",
    "\n",
    "df_user['KG_desire_city'] = '-1'\n",
    "for index, row in df_user.iterrows():\n",
    "    city_ids_str = row['desire_jd_city_id'].split(',')\n",
    "    city_ids = []\n",
    "    for city_id_str in city_ids_str:\n",
    "        if city_id_str == '' or city_id_str == '-':\n",
    "            continue\n",
    "        city_id = int(city_id_str)\n",
    "        if city_id not in city2index:\n",
    "            city2index[city_id] = len(city2index)\n",
    "        city_ids.append(city2index[city_id])\n",
    "    df_user.loc[index, 'KG_desire_city'] = ','.join([str(x) for x in city_ids])\n",
    "\n",
    "df_jd['KG_job_city'] = -1\n",
    "for index, row in df_jd.iterrows():\n",
    "    city_id = int(row['city'])\n",
    "    if city_id not in city2index:\n",
    "        city2index[city_id] = len(city2index)\n",
    "    df_jd.loc[index, 'KG_job_city'] = city2index[city_id]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "industry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry2index = {}\n",
    "\n",
    "df_user['KG_cur_industry'] = -1\n",
    "for index, row in df_user.iterrows():\n",
    "    industry_str = row['cur_industry_id'].strip()\n",
    "    if industry_str == '':\n",
    "        continue\n",
    "    if industry_str not in industry2index:\n",
    "        industry2index[industry_str] = len(industry2index)\n",
    "    df_user.loc[index, 'KG_cur_industry'] = industry2index[industry_str]\n",
    "\n",
    "df_user['KG_desire_industry'] = '-1'\n",
    "for index, row in df_user.iterrows():\n",
    "    industries_str = row['desire_jd_industry_id'].strip()\n",
    "    if industries_str == '':\n",
    "        continue\n",
    "    industries = []\n",
    "    for industry_str in industries_str.split(','):\n",
    "        industry_str = industry_str.strip()\n",
    "        if industry_str == '':\n",
    "            continue\n",
    "        if industry_str not in industry2index:\n",
    "            industry2index[industry_str] = len(industry2index)\n",
    "        industries.append(industry2index[industry_str])\n",
    "    df_user.loc[index, 'KG_desire_industry'] = ','.join([str(x) for x in industries])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "salary:\n",
    "0000000000\t面议\n",
    "0000001000\t1000元以下\n",
    "0100002000\t1000-2000元/月\n",
    "0200104000\t2001-4000元/月\n",
    "0400106000\t4001-6000元/月\n",
    "0600108000\t6001-8000元/月\n",
    "0800110000\t8001-10000元/月\n",
    "100001150000\t100000元以上\n",
    "1000115000\t10001-15000元/月\n",
    "1500120000\t15000-20000元\n",
    "1500125000\t15000-25000元/月\n",
    "2000130000\t20000-30000元\n",
    "2500199999\t25000元/月以上\n",
    "3000150000\t30000-50000元\n",
    "3500150000\t35000-50000元/月\n",
    "5000170000\t50000-70000元/月\n",
    "70001100000\t70000-100000元/月\n",
    "2500135000\t25000-35000元/月"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_salary_dict = {\n",
    "    100002000:1000,\n",
    "    400106000:4000,\n",
    "    200104000:2000,\n",
    "    600108000:6000,\n",
    "    800110000:8000,\n",
    "    1000115000:10000,\n",
    "    2500199999:25000,\n",
    "    1500125000:15000,\n",
    "    3500150000:35000,\n",
    "    70001100000:70000,\n",
    "    1000:0,\n",
    "    100001150000:100000,\n",
    "    2500135000:25000,\n",
    "    5000170000:50000\n",
    "}\n",
    "max_salary_dict = {\n",
    "    100002000:2000,\n",
    "    400106000:6000,\n",
    "    200104000:4000,\n",
    "    600108000:8000,\n",
    "    800110000:10000,\n",
    "    1000115000:15000,\n",
    "    2500199999:99999,\n",
    "    1500125000:25000,\n",
    "    3500150000:50000,\n",
    "    70001100000:100000,\n",
    "    1000:1000,\n",
    "    100001150000:150000,\n",
    "    2500135000:35000,\n",
    "    5000170000:70000\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of salary in df_user: 15\n",
      "num of salary in df_user+df_jd: 53\n"
     ]
    }
   ],
   "source": [
    "salary2index = {}\n",
    "\n",
    "df_user['KG_cur_min_salary'] = -1\n",
    "df_user['KG_cur_max_salary'] = -1\n",
    "for index, row in df_user.iterrows():\n",
    "    cur_salary_id = int(row['cur_salary_id']) if row['cur_salary_id'] not in {'-', ''} else 0\n",
    "    if cur_salary_id not in min_salary_dict:\n",
    "        continue\n",
    "    cur_min_salary = min_salary_dict[cur_salary_id]\n",
    "    if cur_min_salary not in salary2index:\n",
    "        salary2index[cur_min_salary] = len(salary2index)\n",
    "    cur_max_salary = max_salary_dict[cur_salary_id]\n",
    "    if cur_max_salary not in salary2index:\n",
    "        salary2index[cur_max_salary] = len(salary2index)\n",
    "    df_user.loc[index, 'KG_cur_min_salary'] = salary2index[cur_min_salary]\n",
    "    df_user.loc[index, 'KG_cur_max_salary'] = salary2index[cur_max_salary]\n",
    "\n",
    "df_user['KG_desire_min_salary'] = -1\n",
    "df_user['KG_desire_max_salary'] = -1\n",
    "for index, row in df_user.iterrows():\n",
    "    desire_salary_id = int(row['desire_jd_salary_id']) if row['desire_jd_salary_id'] not in {'-', ''} else 0\n",
    "    if desire_salary_id not in min_salary_dict:\n",
    "        continue\n",
    "    desire_min_salary = min_salary_dict[desire_salary_id]\n",
    "    if desire_min_salary not in salary2index:\n",
    "        salary2index[desire_min_salary] = len(salary2index)\n",
    "    desire_max_salary = max_salary_dict[desire_salary_id]\n",
    "    if desire_max_salary not in salary2index:\n",
    "        salary2index[desire_max_salary] = len(salary2index)\n",
    "    df_user.loc[index, 'KG_desire_min_salary'] = salary2index[desire_min_salary]\n",
    "    df_user.loc[index, 'KG_desire_max_salary'] = salary2index[desire_max_salary]\n",
    "\n",
    "print(f\"num of salary in df_user: {len(salary2index)}\")\n",
    "\n",
    "df_jd['KG_min_salary'] = -1\n",
    "df_jd['KG_max_salary'] = -1\n",
    "for index, row in df_jd.iterrows():\n",
    "    max_salary = row['max_salary']\n",
    "    if max_salary == 0:\n",
    "        continue\n",
    "    #max_salary = int(max_salary / 1000) * 1000\n",
    "    #四舍五入\n",
    "    max_salary = int(max_salary / 1000 + 0.5) * 1000\n",
    "    if max_salary not in salary2index:\n",
    "        salary2index[max_salary] = len(salary2index)\n",
    "    min_salary = row['min_salary']\n",
    "    min_salary = int(min_salary / 1000 + 0.5) * 1000\n",
    "    if min_salary not in salary2index:\n",
    "        salary2index[min_salary] = len(salary2index)\n",
    "    \n",
    "    df_jd.loc[index, 'KG_min_salary'] = salary2index[min_salary]\n",
    "    df_jd.loc[index, 'KG_max_salary'] = salary2index[max_salary]\n",
    "\n",
    "print(f\"num of salary in df_user+df_jd: {len(salary2index)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of job types in df_user: 558\n",
      "num of job types in df_user+df_jd: 678\n"
     ]
    }
   ],
   "source": [
    "type2index = {}\n",
    "\n",
    "df_user['KG_cur_jdtype'] = -1\n",
    "for index, row in df_user.iterrows():\n",
    "    type_str = row['cur_jd_type'].strip()\n",
    "    if type_str == '':\n",
    "        continue\n",
    "    if type_str not in type2index:\n",
    "        type2index[type_str] = len(type2index)\n",
    "    df_user.loc[index, 'KG_cur_jdtype'] = type2index[type_str]\n",
    "\n",
    "df_user['KG_desire_jdtype'] = '-1'\n",
    "for index, row in df_user.iterrows():\n",
    "    types_str = row['desire_jd_type_id'].strip()\n",
    "    if types_str == '':\n",
    "        continue\n",
    "    types = []\n",
    "    for type_str in types_str.split(','):\n",
    "        type_str = type_str.strip()\n",
    "        if type_str == '':\n",
    "            continue\n",
    "        if type_str not in type2index:\n",
    "            type2index[type_str] = len(type2index)\n",
    "        types.append(type2index[type_str])\n",
    "    df_user.loc[index, 'KG_desire_jdtype'] = ','.join([str(x) for x in types])\n",
    "\n",
    "print(f\"num of job types in df_user: {len(type2index)}\")\n",
    "\n",
    "df_jd['KG_job_jdtype'] = -1\n",
    "for index, row in df_jd.iterrows():\n",
    "    type_str = row['jd_sub_type'].strip()\n",
    "    if type_str == '' or type_str == '\\\\N':\n",
    "        continue\n",
    "    if type_str not in type2index:\n",
    "        type2index[type_str] = len(type2index)\n",
    "    df_jd.loc[index, 'KG_job_jdtype'] = type2index[type_str]\n",
    "\n",
    "print(f\"num of job types in df_user+df_jd: {len(type2index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_degree = set(['其他','请选择','\\\\N','na', ''])\n",
    "\n",
    "degree2index = {}\n",
    "\n",
    "df_user['KG_cur_degree'] = -1\n",
    "for index, row in df_user.iterrows():\n",
    "    degree_str = row['cur_degree_id'].strip()\n",
    "    if degree_str in noisy_degree:\n",
    "        continue\n",
    "    if degree_str not in degree2index:\n",
    "        degree2index[degree_str] = len(degree2index)\n",
    "    df_user.loc[index, 'KG_cur_degree'] = degree2index[degree_str]\n",
    "\n",
    "df_jd['KG_require_degree'] = -1\n",
    "for index, row in df_jd.iterrows():\n",
    "    degree_str = row['min_edu_level'].strip()\n",
    "    if degree_str in noisy_degree:\n",
    "        continue\n",
    "    if degree_str not in degree2index:\n",
    "        degree2index[degree_str] = len(degree2index)\n",
    "    df_jd.loc[index, 'KG_require_degree'] = degree2index[degree_str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_year_dict = {\n",
    "    305: 4,\n",
    "    1:1,\n",
    "    -1:0,\n",
    "    0:0,\n",
    "    103:2,\n",
    "    510:7,\n",
    "    1099:10,\n",
    "    399:4,\n",
    "    599:7,\n",
    "    199:1,\n",
    "    299:2,\n",
    "    110:1\n",
    "}\n",
    "\n",
    "df_user['KG_cur_year'] = 2019-df_user[\"start_work_date\"].apply(lambda x : 2018 if x==\"-\" else int(x))\n",
    "df_user['KG_cur_year'] = df_user['KG_cur_year'].apply(lambda x : 10 if x>=10 else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jd['KG_require_year'] = df_jd['min_years'].apply(lambda x : min_year_dict[x] if x in min_year_dict else 0)\n",
    "\n",
    "year2index = {}\n",
    "\n",
    "for index, row in df_user.iterrows():\n",
    "    year = row['KG_cur_year']\n",
    "    if year < 0:\n",
    "        year = 0\n",
    "    if year not in year2index:\n",
    "        year2index[year] = len(year2index)\n",
    "    df_user.loc[index, 'KG_cur_year'] = year2index[year]\n",
    "\n",
    "for index, row in df_jd.iterrows():\n",
    "    year = row['KG_require_year']\n",
    "    if year not in year2index:\n",
    "        year2index[year] = len(year2index)\n",
    "    df_jd.loc[index, 'KG_require_year'] = year2index[year]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "wv_from_text = KeyedVectors.load_word2vec_format('/data1/laikaihuang/word_embedding/tencent-ailab-embedding-zh-d200-v0.2.0/tencent-ailab-embedding-zh-d200-v0.2.0_refine.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user['experience'] = df_user['experience'].apply(lambda x : '|'.join(list(set([e.strip() for e in x.split('|') if e.strip() != '']))))\n",
    "df_user['experience'] = df_user['experience'].apply(lambda x : '|'.join([e for e in x.split('|') if e in wv_from_text.key_to_index]))\n",
    "# if more than 30, randomly sample 30\n",
    "df_user['experience'] = df_user['experience'].apply(lambda x : '|'.join(random.sample(x.split('|'), min(30, len(x.split('|'))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp in users: 5507\n"
     ]
    }
   ],
   "source": [
    "noisy_exp = set(['互联网参考模型osi七层','其他','请选择','\\\\N','na', ''])\n",
    "# 加上所有英文字母\n",
    "for i in range(26):\n",
    "    noisy_exp.add(chr(ord('a')+i))\n",
    "    noisy_exp.add(chr(ord('A')+i))\n",
    "    \n",
    "def is_number(s):\n",
    "    if s.isnumeric():\n",
    "        return True\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "import json\n",
    "with open('city.json', 'r', encoding='utf-8') as f:\n",
    "    city_data = json.load(f)\n",
    "\n",
    "provinces = [city_data[key][0]['province'] for key in city_data]\n",
    "provinces = list(set(provinces))\n",
    "cities = [item['name'] for key in city_data for item in city_data[key]]\n",
    "noisy = set(provinces + cities)\n",
    "noisy.update([n[:-1] for n in noisy])\n",
    "noisy.update(['经营范围', '市值', '排名', '其他','要求','学历', '待遇', '处理'])\n",
    "\n",
    "def is_location(s):\n",
    "    if s.endswith('市') or s.endswith('省') or s.endswith('县'):\n",
    "        return True\n",
    "    if s in noisy:\n",
    "        return True\n",
    "    return False    \n",
    "\n",
    "exp2index = {}\n",
    "\n",
    "df_user['KG_cur_experience'] = '-1'\n",
    "for index, row in df_user.iterrows():\n",
    "    exps_str = row['experience']\n",
    "    exps_str = [s.strip() for s in exps_str.split('|')]\n",
    "    exps_str = [s for s in exps_str if s not in noisy_exp and not is_number(s) and not is_location(s)]\n",
    "    exps_str = list(set(exps_str))\n",
    "    experiences = []\n",
    "    for exp_str in exps_str:\n",
    "        if exp_str not in exp2index:\n",
    "            exp2index[exp_str] = len(exp2index)\n",
    "        experiences.append(exp2index[exp_str])\n",
    "    df_user.loc[index, 'KG_cur_experience'] = ','.join([str(x) for x in experiences])\n",
    "\n",
    "print(f\"exp in users: {len(exp2index)}\")\n",
    "user_exp = set(exp2index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19114/19114 [00:00<00:00, 631323.89it/s]\n",
      "100%|██████████| 19114/19114 [00:00<00:00, 48784.01it/s]\n",
      "100%|██████████| 19114/19114 [00:00<00:00, 226775.57it/s]\n",
      "100%|██████████| 19114/19114 [00:00<00:00, 20530.56it/s]\n"
     ]
    }
   ],
   "source": [
    "df_jd_gpt = pd.read_csv('table2_jd_processed_gpt.txt', sep='\\t')\n",
    "df_jd_gpt.fillna('', inplace=True)\n",
    "# 处理GPT提取的关键词\n",
    "# 1、统一分隔符\n",
    "def unify_keyword(keyword):\n",
    "    return keyword.replace('、', '|').replace('，', '|').replace('；', '|').replace(';', '|').replace('、', '|').replace('&', '|')\n",
    "\n",
    "df_jd_gpt['keyword_proc'] = df_jd_gpt['skill_keyword'].progress_apply(unify_keyword)\n",
    "\n",
    "# 2、去掉包含公司名称\n",
    "def clean_keyword(keyword):\n",
    "    keywords = keyword.split('|')\n",
    "    keywords = [k.strip() for k in keywords if k.strip() != '' and not is_number(k.strip())]\n",
    "    keywords = [k for k in keywords if not k.endswith('公司') and not k.endswith('集团') and not k.endswith('县')]\n",
    "    keywords = [k for k in keywords if not '。' in k and not ':' in k and not '：' in k and not '【' in k and not '】' in k]\n",
    "    return '|'.join(list(set(keywords)))\n",
    "\n",
    "df_jd_gpt['keyword_proc'] = df_jd_gpt['keyword_proc'].progress_apply(clean_keyword)\n",
    "\n",
    "# 3、去掉地名和一些无意义的词\n",
    "def clean_noisy(keyword):\n",
    "    keywords = keyword.split('|')\n",
    "    keywords = [k for k in keywords if k not in noisy and len(k) < 10]\n",
    "    return '|'.join(list(set(keywords)))\n",
    "\n",
    "df_jd_gpt['keyword_proc'] = df_jd_gpt['keyword_proc'].progress_apply(clean_noisy)\n",
    "\n",
    "# 4、去掉没有出现在岗位描述中的关键词\n",
    "df_jd_gpt['keyword_proc'] = df_jd_gpt.progress_apply(lambda x: '|'.join([k for k in x['keyword_proc'].split('|') if k in x['job_description']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.215 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "100%|██████████| 19114/19114 [00:02<00:00, 7179.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# 将user中的关键词加入jieba词典，再提取工作描述中的关键词\n",
    "import jieba \n",
    "for word in exp2index:\n",
    "    jieba.add_word(word)\n",
    "\n",
    "df_jd_gpt['keyword_jieba'] = df_jd_gpt['keyword_proc'].progress_apply(lambda x: '|'.join(list(set([c for c in jieba.cut(x) if len(c) > 1 and not is_number(c)]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19114it [00:03, 5552.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp in users+df_jd: 10716\n",
      "exp exist in both user and job: 3148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "job_exp = set()\n",
    "df_jd['KG_require_experience'] = '-1'\n",
    "for index, row in tqdm(df_jd_gpt.iterrows()):\n",
    "    keywords = row['keyword_jieba'].split('|')\n",
    "    keywords = [k for k in keywords if k != '' and k not in noisy and not k.endswith('K') and not k.endswith('-') and not is_number(k) and not k.endswith('%')]\n",
    "\n",
    "    keywords = random.sample(keywords, min(30, len(keywords)))\n",
    "    exps = []\n",
    "    for keyword in keywords:\n",
    "        if keyword in wv_from_text.key_to_index:\n",
    "            job_exp.add(keyword)\n",
    "            if keyword not in exp2index:\n",
    "                exp2index[keyword] = len(exp2index)\n",
    "            exps.append(exp2index[keyword])\n",
    "    df_jd.loc[index, 'KG_require_experience'] = ','.join([str(x) for x in exps])\n",
    "\n",
    "print(f\"exp in users+df_jd: {len(exp2index)}\")\n",
    "word_both = user_exp.intersection(job_exp)\n",
    "print(f\"exp exist in both user and job: {len(word_both)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user['cur_experience_num'] = df_user['KG_cur_experience'].apply(lambda x: len(x.split(',')) if x != '-1' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4500.000000\n",
       "mean       22.411556\n",
       "std         9.160443\n",
       "min         1.000000\n",
       "25%        15.000000\n",
       "50%        29.000000\n",
       "75%        30.000000\n",
       "max        30.000000\n",
       "Name: cur_experience_num, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user['cur_experience_num'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export KG\n",
    "Entity:\n",
    "- user\n",
    "- city\n",
    "- industry\n",
    "- salary\n",
    "- jdtype\n",
    "- degree\n",
    "- year\n",
    "- experience\n",
    "- job\n",
    "- company\n",
    "\n",
    "Relation:\n",
    "- user, cur_city, city\n",
    "- user, desire_city, city\n",
    "- user, cur_industry, industry\n",
    "- user, desire_industry, industry\n",
    "- user, cur_salary, salary\n",
    "- user, desire_salary, salary\n",
    "- user, cur_jdtype, jdtype\n",
    "- user, disire_jdtype, jdtype\n",
    "- user, cur_degree, degree\n",
    "- user, cur_year, year\n",
    "- user, cur_experience, experience\n",
    "- user, desire_job. job \n",
    "  \n",
    "(revserse)\n",
    " \n",
    "- job, job_user, user\n",
    "- job, job_city, city \n",
    "- job, job_type, type\n",
    "- job, require_degree, degree\n",
    "- job, job_salary, salary\n",
    "- job, require_year, year\n",
    "- job, require_experience, experience  \n",
    "\n",
    "(revserse)\n",
    "\n",
    "- degree, higher_degree, degree\n",
    "- degree, lower_degree, degree\n",
    "- salary, higher_salary, salary\n",
    "- salary, lower_salary, salary\n",
    "- year, higher_year, year\n",
    "- year, lower_year, year\n",
    "- experience, similar_experience, experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = {}\n",
    "meta_data['dataset_name'] = 'zhaopin'\n",
    "meta_data['node_data'] = []\n",
    "meta_data['edge_data'] = []\n",
    "node_type_list = ['user', 'job', 'city', 'industry', 'salary', 'jdtype', 'degree', 'year', 'experience']\n",
    "for node_type in node_type_list:\n",
    "    meta_data['node_data'].append({\n",
    "        'file_name': f'entity_{node_type}.csv',\n",
    "        'ntype': node_type\n",
    "    })\n",
    "\n",
    "edge_type_list = [\n",
    "    ('user', 'cur_city', 'city'),\n",
    "    ('user', 'desire_city', 'city'),\n",
    "    ('user', 'cur_industry', 'industry'),\n",
    "    ('user', 'desire_industry', 'industry'),\n",
    "    ('user', 'cur_min_salary', 'salary'),\n",
    "    ('user', 'cur_max_salary', 'salary'),\n",
    "    ('user', 'desire_min_salary', 'salary'),\n",
    "    ('user', 'desire_max_salary', 'salary'),\n",
    "    ('user', 'cur_jdtype', 'jdtype'),\n",
    "    ('user', 'desire_jdtype', 'jdtype'),\n",
    "    ('user', 'cur_degree', 'degree'),\n",
    "    ('user', 'cur_year', 'year'),\n",
    "    ('user', 'cur_experience', 'experience'),\n",
    "    ('user', 'desire_job', 'job'),\n",
    "    ('city', 'job_city_rev', 'job'),\n",
    "    ('city', 'desire_city_rev', 'user'),\n",
    "    ('industry', 'cur_industry_rev', 'user'),\n",
    "    ('industry', 'desire_industry_rev', 'user'),\n",
    "    ('salary', 'cur_min_salary_rev', 'user'),\n",
    "    ('salary', 'cur_max_salary_rev', 'user'),\n",
    "    ('salary', 'desire_min_salary_rev', 'user'),\n",
    "    ('salary', 'desire_max_salary_rev', 'user'),\n",
    "    ('jdtype', 'cur_jdtype_rev', 'user'),\n",
    "    ('jdtype', 'desire_jdtype_rev', 'user'),\n",
    "    ('degree', 'cur_degree_rev', 'user'),\n",
    "    ('year', 'cur_year_rev', 'user'),\n",
    "    ('experience', 'cur_experience_rev', 'user'),\n",
    "    ('job', 'job_user', 'user'),\n",
    "    ('job', 'job_city', 'city'),\n",
    "    ('job', 'min_salary', 'salary'),\n",
    "    ('job', 'max_salary', 'salary'),\n",
    "    ('job', 'job_jdtype', 'jdtype'),\n",
    "    ('job', 'require_degree', 'degree'),\n",
    "    ('job', 'require_year', 'year'),\n",
    "    ('job', 'require_experience', 'experience'),\n",
    "    ('salary', 'min_salary_rev', 'job'),\n",
    "    ('salary', 'max_salary_rev', 'job'),\n",
    "    ('jdtype', 'job_jdtype_rev', 'job'),\n",
    "    ('degree', 'require_degree_rev', 'job'),\n",
    "    ('year', 'require_year_rev', 'job'),\n",
    "    ('experience', 'require_experience_rev', 'job'),\n",
    "    ('degree', 'higher_degree', 'degree'),\n",
    "    ('degree', 'lower_degree', 'degree'),\n",
    "    ('salary', 'higher_salary', 'salary'),\n",
    "    ('salary', 'lower_salary', 'salary'),\n",
    "    ('year', 'higher_year', 'year'),\n",
    "    ('year', 'lower_year', 'year'),\n",
    "    ('experience', 'similar_experience', 'experience'),\n",
    "]\n",
    "\n",
    "\n",
    "for edge_type in edge_type_list:\n",
    "    meta_data['edge_data'].append({\n",
    "        'file_name': f'r_{edge_type[0]}_{edge_type[1]}_{edge_type[2]}.csv',\n",
    "        'etype': [edge_type[0], edge_type[1], edge_type[2]]\n",
    "    })\n",
    "\n",
    "def write_meta_data(meta_data, file_path):\n",
    "    f = open(file_path, 'w')\n",
    "    # dataset_name\n",
    "    dataset_name = meta_data['dataset_name']\n",
    "    f.write(f\"dataset_name: {dataset_name}\\n\")\n",
    "    # edge_data\n",
    "    f.write(\"edge_data:\\n\")\n",
    "    for edge in meta_data['edge_data']:\n",
    "        f.write(f\"- file_name: {edge['file_name']}\\n\")\n",
    "        f.write(f\"  etype: [{','.join(edge['etype'])}]\\n\")\n",
    "    # node_data\n",
    "    f.write(\"node_data:\\n\")\n",
    "    for node in meta_data['node_data']:\n",
    "        f.write(f\"- file_name: {node['file_name']}\\n\")\n",
    "        f.write(f\"  ntype: {node['ntype']}\\n\")\n",
    "        \n",
    "write_meta_data(meta_data, os.path.join(kg_out_dir, 'meta.yaml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export entity\n",
    "# user\n",
    "entity_user = df_user[['user_id']]\n",
    "entity_user.to_csv(os.path.join(kg_text_out_dir, 'entity_user.csv'), index=False)\n",
    "pd.DataFrame({'node_id': range(len(entity_user))}).to_csv(os.path.join(kg_out_dir, 'entity_user.csv'), index=False)\n",
    "# job\n",
    "entity_job = df_jd[['jd_title']]\n",
    "entity_job.to_csv(os.path.join(kg_text_out_dir, 'entity_job.csv'), index=False)\n",
    "pd.DataFrame({'node_id': range(len(entity_job))}).to_csv(os.path.join(kg_out_dir, 'entity_job.csv'), index=False)\n",
    "# city\n",
    "pd.DataFrame({'city_id': list(city2index.keys())}).to_csv(os.path.join(kg_text_out_dir, 'entity_city.csv'), index=False)\n",
    "pd.DataFrame({'node_id': list(city2index.values())}).to_csv(os.path.join(kg_out_dir, 'entity_city.csv'), index=False)\n",
    "# industry\n",
    "pd.DataFrame({'industry_name': list(industry2index.keys())}).to_csv(os.path.join(kg_text_out_dir, 'entity_industry.csv'), index=False)\n",
    "pd.DataFrame({'node_id': list(industry2index.values())}).to_csv(os.path.join(kg_out_dir, 'entity_industry.csv'), index=False)\n",
    "# salary\n",
    "pd.DataFrame({'salary': list(salary2index.keys())}).to_csv(os.path.join(kg_text_out_dir, 'entity_salary.csv'), index=False)\n",
    "pd.DataFrame({'node_id': list(salary2index.values())}).to_csv(os.path.join(kg_out_dir, 'entity_salary.csv'), index=False)\n",
    "# type\n",
    "pd.DataFrame({'type_name': list(type2index.keys())}).to_csv(os.path.join(kg_text_out_dir, 'entity_jdtype.csv'), index=False)\n",
    "pd.DataFrame({'node_id': list(type2index.values())}).to_csv(os.path.join(kg_out_dir, 'entity_jdtype.csv'), index=False)\n",
    "# degree\n",
    "pd.DataFrame({'degree_name': list(degree2index.keys())}).to_csv(os.path.join(kg_text_out_dir, 'entity_degree.csv'), index=False)\n",
    "pd.DataFrame({'node_id': list(degree2index.values())}).to_csv(os.path.join(kg_out_dir, 'entity_degree.csv'), index=False)\n",
    "# year\n",
    "pd.DataFrame({'year': list(year2index.keys())}).to_csv(os.path.join(kg_text_out_dir, 'entity_year.csv'), index=False)\n",
    "pd.DataFrame({'node_id': list(year2index.values())}).to_csv(os.path.join(kg_out_dir, 'entity_year.csv'), index=False)\n",
    "# experience\n",
    "pd.DataFrame({'experience': list(exp2index.keys())}).to_csv(os.path.join(kg_text_out_dir, 'entity_experience.csv'), index=False)\n",
    "pd.DataFrame({'node_id': list(exp2index.values())}).to_csv(os.path.join(kg_out_dir, 'entity_experience.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing relation for user - cur_city - city\n",
      "processing relation for user - desire_city - city\n",
      "processing relation for user - cur_industry - industry\n",
      "processing relation for user - desire_industry - industry\n",
      "processing relation for user - cur_min_salary - salary\n",
      "processing relation for user - cur_max_salary - salary\n",
      "processing relation for user - desire_min_salary - salary\n",
      "processing relation for user - desire_max_salary - salary\n",
      "processing relation for user - cur_jdtype - jdtype\n",
      "processing relation for user - desire_jdtype - jdtype\n",
      "processing relation for user - cur_degree - degree\n",
      "processing relation for user - cur_year - year\n",
      "processing relation for user - cur_experience - experience\n",
      "processing relation for job - job_city - city\n",
      "processing relation for job - min_salary - salary\n",
      "processing relation for job - max_salary - salary\n",
      "processing relation for job - job_jdtype - jdtype\n",
      "processing relation for job - require_degree - degree\n",
      "processing relation for job - require_year - year\n",
      "processing relation for job - require_experience - experience\n"
     ]
    }
   ],
   "source": [
    "# export relation\n",
    "for dataframe in [df_user, df_jd]:\n",
    "    for colomn in dataframe.columns:\n",
    "        if colomn.startswith('KG_'):\n",
    "            src_type = dataframe.columns[0][:-3]\n",
    "            src_type = 'job' if src_type == 'jd' else src_type\n",
    "            relation_type = colomn[3:]\n",
    "            dst_type = relation_type.split('_')[-1]\n",
    "            print(f\"processing relation for {src_type} - {relation_type} - {dst_type}\")\n",
    "            df_temp = dataframe[[colomn]].astype(str)\n",
    "            df_out = pd.DataFrame(columns=['src_id', 'dst_id'])\n",
    "            for index, row in df_temp.iterrows():\n",
    "                src_id = index\n",
    "                dst_ids = row[colomn].split(',')\n",
    "                for dst_id in dst_ids:\n",
    "                    if dst_id == '' or dst_id == '-1':\n",
    "                        continue\n",
    "                    dst_id = int(dst_id)\n",
    "                    df_out = pd.concat([df_out, pd.DataFrame({'src_id': [src_id], 'dst_id': [dst_id]})], ignore_index=True)\n",
    "\n",
    "            df_out.to_csv(os.path.join(kg_out_dir, f'r_{src_type}_{relation_type}_{dst_type}.csv'), index=False)\n",
    "            # reverse\n",
    "            df_out = df_out[['dst_id', 'src_id']]\n",
    "            df_out.rename(columns={'dst_id': 'src_id', 'src_id': 'dst_id'}, inplace=True)\n",
    "            df_out.to_csv(os.path.join(kg_out_dir, f'r_{dst_type}_{relation_type}_rev_{src_type}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree_higher_degree_degree\n",
    "degree_grade = ['初中', '中专', '高中', '中技', '大专', '本科', '硕士', '博士']\n",
    "df_out = pd.DataFrame(columns=['src_id', 'dst_id'])\n",
    "for i in range(len(degree_grade)):\n",
    "    for j in range(i+1, len(degree_grade)):\n",
    "        df_out = pd.concat([df_out, pd.DataFrame({'src_id': [degree2index[degree_grade[i]]], 'dst_id': [degree2index[degree_grade[j]]]})], ignore_index=True)\n",
    "df_out.to_csv(os.path.join(kg_out_dir, f'r_degree_higher_degree_degree.csv'), index=False)\n",
    "\n",
    "# degree_lower_degree_degree\n",
    "df_out = df_out[['dst_id', 'src_id']]\n",
    "df_out.rename(columns={'dst_id': 'src_id', 'src_id': 'dst_id'}, inplace=True)\n",
    "df_out.to_csv(os.path.join(kg_out_dir, f'r_degree_lower_degree_degree.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salary_higher_salary_salary\n",
    "salarys = list(salary2index.keys())\n",
    "salarys.sort()\n",
    "df_out = pd.DataFrame(columns=['src_id', 'dst_id'])\n",
    "for i in range(len(salarys)):\n",
    "    for j in range(i+1, len(salarys)):\n",
    "        df_out = pd.concat([df_out, pd.DataFrame({'src_id': [salary2index[salarys[i]]], 'dst_id': [salary2index[salarys[j]]]})], ignore_index=True)\n",
    "df_out.to_csv(os.path.join(kg_out_dir, f'r_salary_higher_salary_salary.csv'), index=False)\n",
    "\n",
    "# salary_lower_salary_salary\n",
    "df_out = df_out[['dst_id', 'src_id']]\n",
    "df_out.rename(columns={'dst_id': 'src_id', 'src_id': 'dst_id'}, inplace=True)\n",
    "df_out.to_csv(os.path.join(kg_out_dir, f'r_salary_lower_salary_salary.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year_higher_year_year\n",
    "years = list(year2index.keys())\n",
    "years.sort()\n",
    "df_out = pd.DataFrame(columns=['src_id', 'dst_id'])\n",
    "for i in range(len(years)):\n",
    "    for j in range(i+1, len(years)):\n",
    "        df_out = pd.concat([df_out, pd.DataFrame({'src_id': [year2index[years[i]]], 'dst_id': [year2index[years[j]]]})], ignore_index=True)\n",
    "df_out.to_csv(os.path.join(kg_out_dir, f'r_year_higher_year_year.csv'), index=False)\n",
    "\n",
    "# year_lower_year_year\n",
    "df_out = df_out[['dst_id', 'src_id']]\n",
    "df_out.rename(columns={'dst_id': 'src_id', 'src_id': 'dst_id'}, inplace=True)\n",
    "df_out.to_csv(os.path.join(kg_out_dir, f'r_year_lower_year_year.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_desire_job_job\n",
    "train_links = np.load('train_links.npy')\n",
    "train_users = train_links[0]\n",
    "train_items = train_links[1]\n",
    "user_id2index = {user_id: index for index, user_id in enumerate(df_user['user_id'].tolist())}\n",
    "item_id2index = {item_id: index for index, item_id in enumerate(df_jd['jd_no'].tolist())}\n",
    "train_user_index = np.array([user_id2index[user_id] for user_id in train_users])\n",
    "train_item_index = np.array([item_id2index[item_id] for item_id in train_items])\n",
    "\n",
    "df_out = pd.DataFrame({'src_id': train_user_index.tolist(), 'dst_id': train_item_index.tolist()})\n",
    "df_out.to_csv(os.path.join(kg_out_dir, f'r_user_desire_job_job.csv'), index=False)\n",
    "\n",
    "df_out = pd.DataFrame({'src_id': train_item_index.tolist(), 'dst_id': train_user_index.tolist()})\n",
    "df_out.to_csv(os.path.join(kg_out_dir, f'r_job_job_user_user.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_sim_exp\n",
    "exp_vectors = np.zeros((len(exp2index), 200))\n",
    "for exp, i in exp2index.items():\n",
    "    exp_vectors[i] = wv_from_text[exp]\n",
    "\n",
    "# 计算两两之间的相似度\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity_matrix = cosine_similarity(exp_vectors)\n",
    "\n",
    "# 如果相似度大于0.75，则认为是相似的\n",
    "df_out = pd.DataFrame(columns=['src_id', 'dst_id'])\n",
    "for i in range(len(exp2index)):\n",
    "    for j in range(i+1, len(exp2index)):\n",
    "        if similarity_matrix[i][j] > 0.75:\n",
    "            df_out = pd.concat([df_out, pd.DataFrame({'src_id': [i], 'dst_id': [j]})], ignore_index=True)\n",
    "\n",
    "df_out = pd.concat([df_out, df_out[['dst_id', 'src_id']].rename(columns={'dst_id': 'src_id', 'src_id': 'dst_id'})], ignore_index=True)\n",
    "df_out.to_csv(os.path.join(kg_out_dir, f'r_experience_similar_experience_experience.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out['src_word'] = df_out['src_id'].apply(lambda x: list(exp2index.keys())[x])\n",
    "df_out['dst_word'] = df_out['dst_id'].apply(lambda x: list(exp2index.keys())[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
